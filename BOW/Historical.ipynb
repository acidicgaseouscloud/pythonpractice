{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification with a Bag-of-Words model\n",
    "==============================================\n",
    "\n",
    "This notebook demonstrates how we can train a classifier to predict a label for a text based on word counts.\n",
    "\n",
    "The texts consist of 19th century novels in English from project Gutenberg. The selection is based on the \"unsuccessful\" and successful novels (low and high download counts) as used in Ashok et al (2013), [Success with style](http://aclweb.org/anthology/D13-1181).\n",
    "\n",
    "We will consider the same classification task of successful vs unsuccessful novels.\n",
    "\n",
    "First some preliminaries. Import the external libraries we will use.\n",
    "Most importantly, we rely on Scikit-Learn to do machine learning. See http://scikit-learn.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.decomposition\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "\n",
    "# Tweak how tables are displayed\n",
    "pandas.set_option('display.precision', 4)\n",
    "pandas.set_option('display.max_colwidth', 30)\n",
    "pandas.set_option('display.colheader_justify', 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the texts and convert to a bag-of-words table\n",
    "--------------------------------------------------\n",
    "The bag-of-words ([BOW](http://en.wikipedia.org/wiki/Bag-of-words_model)) table will contain a count for each text and every word.\n",
    "\n",
    "All letters are converted to lower case and we filter the words until we only have words with two or more alphanumeric characters (no punctuation).\n",
    "\n",
    "Important parameters are:\n",
    "\n",
    "-  ``max_features`` limits the model to consider only the top *n* most frequent words. More is often better but may be slow to compute.\n",
    "- ``min_df`` and ``max_df`` restrict the words to those that occur in a certain proportion of texts.\n",
    "\n",
    "For example, setting these to the values 0.2 and 0.8, respectively, restricts the model to words that are in at least 20%, and at most 80% of the texts. This removes rare words on the one hand, and ignores highly frequent words such as [function words](http://en.wikipedia.org/wiki/Function_word) on the other.\n",
    "\n",
    "- ``use_idf=True`` turns on [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) weighting; this incorporates not only the total frequency of a word, but also the number of texts in which it appears. This means that words which are frequent in one text but not in others will get a high score, while words that are frequent in all documents receive a lower score.\n",
    "- ``sublinear_tf=True`` is another variation which makes the word frequencies logarithmic.\n",
    "- ``ngram_range``: selects the number of words in each feature; the default of ``(1, 1)`` selects individual words, which is most efficient. Higher numbers result in more informative features, but also requires a lot more memory. Bigrams, i.e., ``ngram_range=(2, 2)`` is a good trade-off and yields features with 2 consecutive words such as ``the man`` and ``man walks`` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transform() vs fit_transform()\n",
    "\n",
    "\n",
    "fit means to fit the model to the data being provided. This is where the model \"learns\" from the data.\n",
    "\n",
    "transform means to transform the data (produce model outputs) according to the fitted model.\n",
    "\n",
    "fit_transform means to do both - Fit the model to the data, then transform the data according to the fitted model. Calling fit_transform is a convenience to avoid needing to call fit and transform sequentially on the same input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a simple BOW model\n",
    "# vectorizer is a variable that contains the instructions on how to read the text\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "        input='filename', lowercase=True, token_pattern=r'\\b[-\\w]{1,}\\b',\n",
    "        min_df=0.2, max_df=0.8, max_features=10000,\n",
    "        use_idf=True, sublinear_tf=False, ngram_range=(2, 3)) # two args min and max value\n",
    "\n",
    "# max features refers to analyse the top 10000 ngrams. So it will count all the ngrams\n",
    "# and give you the top 10000\n",
    "# Get a list of all filenames in the 'train/' folder,\n",
    "# and add their text to the BOW table 'X'.\n",
    "histpd = pandas.read_csv(\"historical.csv\")\n",
    "histpd = pandas.read_csv('historical.csv')\n",
    "filenames = list(histpd[\"Filename\"])\n",
    "\n",
    "#filenames = os.listdir('train/')\n",
    "\n",
    "ft = []\n",
    "\n",
    "for a in filenames:\n",
    "    if a in os.listdir('train/'):\n",
    "        ft.append(a)\n",
    "\n",
    "# X = vectorizer.fit_transform(['train/' + a for a in filenames])\n",
    "X = vectorizer.fit_transform(['train/' + a for a in ft])\n",
    "\n",
    "# X contains the read text\n",
    "\n",
    "# .fit_transform() Learn the vocabulary dictionary and return document-term matrix.\n",
    "# vectorizer.fit_transform(...) collects the vocabulary of words/ngrams (the fit part),\n",
    "# and converts the texts to a table of numbers (the transform part).\n",
    "# the result of that is put in the variable X, which is just a \n",
    "# table of numbers, but the vectorizer knows which columns refer to which words/ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) term frequencyâ€“inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "At initialization, vectorizer is empty. after you fit it on a corpus (line 10), yes, it has a list of cleaned ngrams and it knows how to convert a text to a vector of counts referring to those ngrams. When we initialise it we are just telling vectorizer how to read the text and what to store. \n",
    "\n",
    "the same thing happens later with the classifier. it is only trained when you use .fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 660)\t0.001618365771111227\n",
      "  (0, 4018)\t0.0014541083260844203\n",
      "  (0, 3721)\t0.0022084084026027114\n",
      "  (0, 4511)\t0.0014169248873963317\n",
      "  (0, 5431)\t0.0014541083260844203\n",
      "  (0, 7033)\t0.001574713760251304\n",
      "  (0, 4988)\t0.0013988372256013594\n",
      "  (0, 2751)\t0.0015963025078015506\n",
      "  (0, 1091)\t0.002946459258736117\n",
      "  (0, 1656)\t0.00205941875105873\n",
      "  (0, 3452)\t0.0017620158458614853\n",
      "  (0, 7645)\t0.0025612042607286944\n",
      "  (0, 4222)\t0.0014732296293680585\n",
      "  (0, 6586)\t0.0012966638821470084\n",
      "  (0, 9373)\t0.001618365771111227\n",
      "  (0, 7054)\t0.0015126008743788454\n",
      "  (0, 9860)\t0.0015126008743788454\n",
      "  (0, 3278)\t0.0015963025078015506\n",
      "  (0, 9867)\t0.0014927222432053588\n",
      "  (0, 7466)\t0.0023367324075895916\n",
      "  (0, 5751)\t0.0014353444513501817\n",
      "  (0, 4554)\t0.0017620158458614853\n",
      "  (0, 3006)\t0.0029854444864107176\n",
      "  (0, 7330)\t0.0014732296293680585\n",
      "  (0, 9092)\t0.0018423657630806207\n",
      "  :\t:\n",
      "  (81, 5064)\t0.0032532060309458377\n",
      "  (81, 1181)\t0.003084952763285674\n",
      "  (81, 7151)\t0.003084952763285674\n",
      "  (81, 2789)\t0.005167604757226258\n",
      "  (81, 9099)\t0.002855411081934474\n",
      "  (81, 1320)\t0.0026158952718894637\n",
      "  (81, 9216)\t0.0032532060309458377\n",
      "  (81, 1627)\t0.009254858289857022\n",
      "  (81, 7225)\t0.009254858289857022\n",
      "  (81, 5937)\t0.012839453554520078\n",
      "  (81, 7748)\t0.023736402035935477\n",
      "  (81, 9170)\t0.011136594158198508\n",
      "  (81, 4321)\t0.018086616650291903\n",
      "  (81, 626)\t0.002891969284756794\n",
      "  (81, 5435)\t0.005934100508983869\n",
      "  (81, 3915)\t0.0034361077458881445\n",
      "  (81, 8195)\t0.01464587284453696\n",
      "  (81, 5099)\t0.004241174188585714\n",
      "  (81, 6932)\t0.0036364539068246427\n",
      "  (81, 2631)\t0.008352445618648882\n",
      "  (81, 2669)\t0.00784768581566839\n",
      "  (81, 5952)\t0.005231790543778927\n",
      "  (81, 490)\t0.0030449126254642636\n",
      "  (81, 6108)\t0.005568297079099254\n",
      "  (81, 9533)\t0.0077514071358393884\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the vector representation\n",
    "---------------------------------\n",
    "To see what the vector representation looks like, we will look at how a simple [example](https://en.wikipedia.org/wiki/It_was_a_dark_and_stormy_night) is transformed to the BOW representation.\n",
    "\n",
    "Notice that because of the parameters before, many words are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>index</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when it was</td>\n",
       "      <td>(0, 9404)</td>\n",
       "      <td>0.2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the streets</td>\n",
       "      <td>(0, 7958)</td>\n",
       "      <td>0.2757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the rain</td>\n",
       "      <td>(0, 7805)</td>\n",
       "      <td>0.3015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that our</td>\n",
       "      <td>(0, 7037)</td>\n",
       "      <td>0.2757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of wind</td>\n",
       "      <td>(0, 5579)</td>\n",
       "      <td>0.3451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it is in</td>\n",
       "      <td>(0, 4226)</td>\n",
       "      <td>0.3179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>in london</td>\n",
       "      <td>(0, 3954)</td>\n",
       "      <td>0.4066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for it is</td>\n",
       "      <td>(0, 2351)</td>\n",
       "      <td>0.3095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fell in</td>\n",
       "      <td>(0, 2244)</td>\n",
       "      <td>0.2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a violent</td>\n",
       "      <td>(0, 306)</td>\n",
       "      <td>0.3763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word         index       weight\n",
       "0  when it was  (0, 9404)  0.2590\n",
       "1  the streets  (0, 7958)  0.2757\n",
       "2     the rain  (0, 7805)  0.3015\n",
       "3     that our  (0, 7037)  0.2757\n",
       "4      of wind  (0, 5579)  0.3451\n",
       "5     it is in  (0, 4226)  0.3179\n",
       "6    in london  (0, 3954)  0.4066\n",
       "7    for it is  (0, 2351)  0.3095\n",
       "8      fell in  (0, 2244)  0.2590\n",
       "9    a violent   (0, 306)  0.3763"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ('It was a dark and stormy night; '\n",
    "        'the rain fell in torrents -- except at occasional intervals, '\n",
    "        'when it was checked by a violent gust of wind which swept '\n",
    "        'up the streets (for it is in London that our scene lies), '\n",
    "        'rattling along the housetops, and fiercely agitating the scanty '\n",
    "        'flame of the lamps that struggled against the darkness.')\n",
    "with open('darkandstormy.txt', 'w', encoding='utf8') as out:\n",
    "    out.write(text)\n",
    "vec = vectorizer.transform(['darkandstormy.txt']) # Transform documents to document-term matrix. No training.\n",
    "\n",
    "# we get back a large vector with a value for each possible word.\n",
    "# show a table with only the non-zero items in the vector: \n",
    "feature_names = vectorizer.get_feature_names() # gets the actual words\n",
    "pandas.DataFrame([(feature_names[b], (a, b), vec[a, b])\n",
    "                  # from dictionary giving the word (value) , then using the (key) index\n",
    "                        for a, b in zip(*vec.nonzero())], \n",
    "                       columns=['word', 'index', 'weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-zero items will exist for a document because it stores the words from previous documents it has read. \n",
    "\n",
    "The Index column refers to the index of the file and the index of the word, or more generally, the row and column in the array/matrix/table\n",
    "\n",
    "[`zip()`](https://www.geeksforgeeks.org/zip-in-python/) maps the separate values into one tuple containing all aspects of the item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get label for each text\n",
    "----------------------\n",
    "The genre of each text, whether a text is a \"success\" or a \"failure\" (based on download counts), and some other information, is specified in a separate metadata file.\n",
    "\n",
    "Note that in the original data, a text may have multiple genres, and an arbitrary genre was picked in this case. In a more careful study the single most appropriate genre would have to be selected by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename,Label,Genre,Fold,Success,Title,Author,Language,DownloadCount\n",
      "11228.txt,Chesnutt_TheMarrowOfTradition,Historical,1,SUCCESS,the marrow of tradition,\"chesnutt, charles w. (charles waddell), 1858-1932\",en,147\n",
      "11413.txt,Doyle_TheRefugeesATaleOfTwoContinents,Historical,1,SUCCESS,the refugees a tale of two continents,\"doyle, arthur conan, sir, 1859-1930\",en,129\n",
      "17221.txt,Defoe_HistoryOfThePlagueInLondon,Historical,1,SUCCESS,history of the plague in london,\"defoe, daniel, 1661?-1731\",en,247\n",
      "1880.txt,Cooper_PathfinderOrTheInlandSea,Historical,1,SUCCESS,\"pathfinder; or, the inland sea\",\"cooper, james fenimore, 1789-1851\",en,185\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 lines to see what the metadata looks like:\n",
    "for line in list(open('historical.csv', encoding='utf8'))[:5]:\n",
    "    print(line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the books are assigned to 5 \"folds\" randomly, to divide the corpus into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS'\n",
      " 'SUCCESS' 'SUCCESS' 'SUCCESS' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE'\n",
      " 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'SUCCESS'\n",
      " 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS'\n",
      " 'SUCCESS' 'SUCCESS' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE'\n",
      " 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'SUCCESS' 'SUCCESS'\n",
      " 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS'\n",
      " 'SUCCESS' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE'\n",
      " 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'SUCCESS' 'SUCCESS' 'SUCCESS'\n",
      " 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS'\n",
      " 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE'\n",
      " 'FAILURE' 'FAILURE' 'FAILURE' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS'\n",
      " 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'SUCCESS' 'FAILURE'\n",
      " 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE' 'FAILURE'\n",
      " 'FAILURE' 'FAILURE']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data; metadata.index will be the filename.\n",
    "metadata = pandas.read_csv('historical.csv', index_col='Filename', encoding='utf8')\n",
    "labels = dict(zip(metadata.index, metadata['Success']))\n",
    "\n",
    "# collect the labels we want to predict\n",
    "y = numpy.array([labels[a] for a in filenames])\n",
    "print(y)\n",
    "# Create an abbreviated label \"Author_Title\" for each text\n",
    "authors = dict(zip(metadata.index, metadata['Author']))\n",
    "titles = dict(zip(metadata.index, metadata['Title']))\n",
    "abbrtitles = ['%s_%s' % (authors[a].split(',')[0].title(),\n",
    "        titles[a][:15].title()) for a in filenames]\n",
    "\n",
    "type(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier\n",
    "------------------\n",
    "\n",
    "You can try different values for the parameter ``C``.\n",
    "This parameter controls the level of regularization;\n",
    "with higher values, the model will take more edge cases\n",
    "(datapoints close to datapoints of other classes) into account.\n",
    "This will give better scores on data that is similar to the training data,\n",
    "but if the training data is not representative, it may result in more errors.\n",
    "\n",
    "regularisation: ignore certain outliers. \n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "[Logistic Regression Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)in its basic form, models a binary dependent variable (win/lose, pass/fail). Models the probability of a certain event existing.\n",
    "The function that converts log-odds to probability is the logistic function. \n",
    "\n",
    "The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (which group does a new data point belong to?) (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.\n",
    "\n",
    "As opposed to a linear regression model which fits the data to a straight line, a logistic regression model fits the data to a logistic regression curve, which looks like an asymptote. |S|\n",
    "\n",
    "Multinomial regression uses a dependent variable with more than two categories. \n",
    "\n",
    "## lbgfs\n",
    "\n",
    "solver{â€˜newton-cgâ€™, â€˜lbfgsâ€™, â€˜liblinearâ€™, â€˜sagâ€™, â€˜sagaâ€™}, default=â€™lbfgsâ€™\n",
    "\n",
    "Algorithm to use in the optimization problem: \n",
    "\n",
    "- For small datasets, â€˜liblinearâ€™ is a good choice, whereas â€˜sagâ€™ and â€˜sagaâ€™ are faster for large ones.\n",
    "\n",
    "- For multiclass problems, only â€˜newton-cgâ€™, â€˜sagâ€™, â€˜sagaâ€™ and â€˜lbfgsâ€™ handle multinomial loss; â€˜liblinearâ€™ is limited to one-versus-rest schemes.\n",
    "\n",
    "- â€˜newton-cgâ€™, â€˜lbfgsâ€™, â€˜sagâ€™ and â€˜sagaâ€™ handle L2 or no penalty\n",
    "\n",
    "- â€˜liblinearâ€™ and â€˜sagaâ€™ also handle L1 penalty\n",
    "\n",
    "- â€˜sagaâ€™ also supports â€˜elasticnetâ€™ penalty\n",
    "\n",
    "[lbfgs](https://en.wikipedia.org/wiki/Limited-memory_BFGS): The algorithm's target problem is to minimize f(x) over unconstrained values (x can have any value) of the real-vector x where f is a differentiable scalar function (continuous). Trying to find the minimum value of the function. In this case, reducing the amount of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index (99) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-6adfd06287fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fit => learning from the data then it predicts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Overall accuracy: %g %%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Dispatch to specialized methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_asindices\u001b[0;34m(self, idx, length)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mmax_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_indx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mmin_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index (99) out of range"
     ]
    }
   ],
   "source": [
    "# Use the \"Fold\" column to split the training data into a train and test set.\n",
    "# These folds were chosen in such a way that the labels are well balanced\n",
    "# and all works by each other only occur in a single fold.\n",
    "folds = dict(zip(metadata.index, metadata['Fold']))\n",
    "\n",
    "# sss contains the split up sample\n",
    "sss = sklearn.model_selection.PredefinedSplit([folds[a] for a in filenames])\n",
    "# This returns train_index and test_index, which are arrays with indices of the datapoints\n",
    "# that should be used for training and testing, respectively.\n",
    "\n",
    "\n",
    "# Train a linear classifier and predict the genre of the items in the test set. \n",
    "# predict the genre?\n",
    "# as before, giving instructions on how to read\n",
    "clf = sklearn.linear_model.LogisticRegression(C=3, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# X refers to the document-term matrix with the list of indices of cleaned ngrams\n",
    "# X is a table of word counts, one document per row, one word/ngram per column.\n",
    "# X[train_index] then selects only the word counts for the books that should be \n",
    "# trained on, without the books in test_index.\n",
    "# y refers to the numpy array of title names of the texts in the corpus and whether they fail or succeed\n",
    "\n",
    "# when we say sss.split(X,y) we are saying split X and y based on the instructions stored in sss\n",
    "for train_index, test_index in sss.split(X, y): \n",
    "    clf.fit(X[train_index], y[train_index]) # fit => learning from the data then it predicts.\n",
    "    pred = clf.predict(X[test_index]) \n",
    "    prob = clf.predict_proba(X[test_index])\n",
    "print('Overall accuracy: %g %%' % sklearn.metrics.accuracy_score(y[test_index], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FAILURE', 'SUCCESS'], dtype='<U7')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the classifier\n",
    "-----------------------\n",
    "The breakdown shows that not all genres are predicted as well;\n",
    "the f-score column is the most important.\n",
    "\n",
    "In the [confusion matrix](http://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "we can see which genres were mistaken most often.\n",
    "The columns hold the number of times the model predicted a genre,\n",
    "while the rows show the true genres.\n",
    "\n",
    "specifically, the diagonal from the top left to the bottom right are the counts of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [20, 21]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-323073be9ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m pandas.DataFrame(sklearn.metrics.confusion_matrix(y[test_index], pred),\n\u001b[1;32m      3\u001b[0m                  index=clf.classes_, columns=clf.classes_)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1927\u001b[0m     \"\"\"\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 256\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [20, 21]"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y[test_index], pred))\n",
    "pandas.DataFrame(sklearn.metrics.confusion_matrix(y[test_index], pred),\n",
    "                 index=clf.classes_, columns=clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words are most strongly associated with each genre?\n",
    "---------------------------------------------------------\n",
    "For each genre, the top 10 words most strongly linked to each genre are shown.\n",
    "\n",
    "The words are ordered by the weight of the model for each genre combined with the average frequency of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the weights of the classifier and take top 10 items\n",
    "topfeatures = {}\n",
    "avgfreq = numpy.squeeze(numpy.asarray(X.mean(axis=0)))\n",
    "if len(clf.classes_) > 2:\n",
    "    for n, target in enumerate(clf.classes_):\n",
    "        top10 = numpy.argsort(clf.coef_[n] * avgfreq)[::-1][:10]\n",
    "        topfeatures[target] = pandas.DataFrame({\n",
    "                'word': [feature_names[m] for m in top10],\n",
    "                'score': (clf.coef_[n] * avgfreq)[top10]},\n",
    "                index=range(1, 11))\n",
    "else:\n",
    "    # in case of a binary classification, negative weights are for the first class,\n",
    "    # positive weights for the second\n",
    "    top10 = numpy.argsort(clf.coef_[0] * avgfreq)[:10]\n",
    "    topfeatures[clf.classes_[0]] = pandas.DataFrame({\n",
    "            'word': [feature_names[m] for m in top10],\n",
    "            'score': (clf.coef_[0] * avgfreq)[top10]},\n",
    "            index=range(1, 11))\n",
    "    top10 = numpy.argsort(clf.coef_[0] * avgfreq)[::-1][:10]\n",
    "    topfeatures[clf.classes_[1]] = pandas.DataFrame({\n",
    "            'word': [feature_names[m] for m in top10],\n",
    "            'score': (clf.coef_[0] * avgfreq)[top10]},\n",
    "            index=range(1, 11))\n",
    "pandas.concat(topfeatures, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can the model predict labels for new texts?\n",
    "---------------------------------------------\n",
    "\n",
    "If you're done experimenting with the model parameters, we can load new texts that the model has never seen before, and see what it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Since we now evaluate on an external held-out set,\n",
    "# we can use everything as training data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Transform the new files to the format of the existing BOW table\n",
    "newfiles = os.listdir('test/')\n",
    "X1 = vectorizer.transform(['test/' + a for a in newfiles])\n",
    "y1 = numpy.array([labels[a] for a in newfiles])\n",
    "pred = clf.predict(X1)\n",
    "prob = clf.predict_proba(X1)\n",
    "\n",
    "# Evaluate\n",
    "print(sklearn.metrics.classification_report(y1, pred))\n",
    "pandas.DataFrame(sklearn.metrics.confusion_matrix(y1, pred),\n",
    "                       index=clf.classes_,\n",
    "                       columns=clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which books were the hardest to classify?\n",
    "-----------------------------------------\n",
    "\n",
    "The following table lists the Adventure books ordered by how confident the classifier is; i.e., ordered by the probability for the most likely label. A probability ranges from 0 to 1.\n",
    "\n",
    "In the table after this the probabilities for each possible genre are given, and the probabilities of each row add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pandas.DataFrame([\n",
    "            (authors[a].title(), titles[a].title(), p, labels[a], b)\n",
    "        for a, b, p in zip(newfiles, pred, prob.max(axis=1))],\n",
    "        index=newfiles,\n",
    "        columns=['Author', 'Title', 'prob', 'actual', 'predicted'])\n",
    "result.sort_values(by='prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show probabilities of all labels for each novel (in case of 2 labels, this doesn't give extra information)\n",
    "x = pandas.DataFrame(prob, index=newfiles, columns=clf.classes_)\n",
    "x.loc[x.max(axis=1).sort_values().index, :]  # order from lowest to highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words are most strongly associated with each label?\n",
    "---------------------------------------------------------\n",
    "For each label, the top 10 words most strongly linked to each label are shown.\n",
    "\n",
    "The words are ordered by the weight of the model for each label combined with the average frequency of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the weights of the classifier and take top 10 items\n",
    "topfeatures = {}\n",
    "avgfreq = numpy.squeeze(numpy.asarray(X.mean(axis=0)))\n",
    "if len(clf.classes_) > 2:\n",
    "    for n, target in enumerate(clf.classes_):\n",
    "        top10 = numpy.argsort(clf.coef_[n] * avgfreq)[::-1][:10]\n",
    "        topfeatures[target] = pandas.DataFrame({\n",
    "                'word': [feature_names[m] for m in top10],\n",
    "                'score': (clf.coef_[n] * avgfreq)[top10]},\n",
    "                index=range(1, 11))\n",
    "else:\n",
    "    # in case of a binary classification, negative weights are for the first class,\n",
    "    # positive weights for the second\n",
    "    top10 = numpy.argsort(clf.coef_[0] * avgfreq)[:10]\n",
    "    topfeatures[clf.classes_[0]] = pandas.DataFrame({\n",
    "            'word': [feature_names[m] for m in top10],\n",
    "            'score': (clf.coef_[0] * avgfreq)[top10]},\n",
    "            index=range(1, 11))\n",
    "    top10 = numpy.argsort(clf.coef_[0] * avgfreq)[::-1][:10]\n",
    "    topfeatures[clf.classes_[1]] = pandas.DataFrame({\n",
    "            'word': [feature_names[m] for m in top10],\n",
    "            'score': (clf.coef_[0] * avgfreq)[top10]},\n",
    "            index=range(1, 11))\n",
    "pandas.concat(topfeatures, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 =\"# Sort the weights of the classifier and take top 10 itemstopfeatures = {}avgfreq = numpy.squeeze(numpy.asarray(X.mean(axis=0)))if len(clf.classes_) > 2:for n, target in enumerate(clf.classes_):top10 = numpy.argsort(clf.coef_[n] * avgfreq)[::-1][:10]topfeatures[target] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[n] * avgfreq)[top10]},index=range(1, 11))else:# in case of a binary classification, negative weights are for the first class,# positive weights for the secondtop10 = numpy.argsort(clf.coef_[0] * avgfreq)[:10]topfeatures[clf.classes_[0]] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[0] * avgfreq)[top10]},index=range(1, 11))top10 = numpy.argsort(clf.coef_[0] * avgfreq)[::-1][:10]topfeatures[clf.classes_[1]] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[0] * avgfreq)[top10]},index=range(1, 11))pandas.concat(topfeatures, axis=1)\"\n",
    "text2 =\"# Sort the weights of the classifier and take top 10 itemstopfeatures = {}avgfreq = numpy.squeeze(numpy.asarray(X.mean(axis=0)))if len(clf.classes_) > 2:for n, target in enumerate(clf.classes_):top10 = numpy.argsort(clf.coef_[n] * avgfreq)[::-1][:10]topfeatures[target] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[n] * avgfreq)[top10]},index=range(1, 11))else:# in case of a binary classification, negative weights are for the first class,# positive weights for the secondtop10 = numpy.argsort(clf.coef_[0] * avgfreq)[:10]topfeatures[clf.classes_[0]] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[0] * avgfreq)[top10]},index=range(1, 11))top10 = numpy.argsort(clf.coef_[0] * avgfreq)[::-1][:10]topfeatures[clf.classes_[1]] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[0] * avgfreq)[top10]},index=range(1, 11))pandas.concat(topfeatures, axis=1)\"\n",
    "\n",
    "if text1 == text2: \n",
    "    print(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
