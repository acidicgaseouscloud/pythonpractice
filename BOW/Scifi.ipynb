{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification with a Bag-of-Words model\n",
    "==============================================\n",
    "\n",
    "This notebook demonstrates how we can train a classifier to predict a label for a text based on word counts.\n",
    "\n",
    "The texts consist of 19th century novels in English from project Gutenberg. The selection is based on the \"unsuccessful\" and successful novels (low and high download counts) as used in Ashok et al (2013), [Success with style](http://aclweb.org/anthology/D13-1181).\n",
    "\n",
    "We will consider the same classification task of successful vs unsuccessful novels.\n",
    "\n",
    "First some preliminaries. Import the external libraries we will use.\n",
    "Most importantly, we rely on Scikit-Learn to do machine learning. See http://scikit-learn.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.decomposition\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "\n",
    "# Tweak how tables are displayed\n",
    "pandas.set_option('display.precision', 4)\n",
    "pandas.set_option('display.max_colwidth', 30)\n",
    "pandas.set_option('display.colheader_justify', 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the texts and convert to a bag-of-words table\n",
    "--------------------------------------------------\n",
    "The bag-of-words ([BOW](http://en.wikipedia.org/wiki/Bag-of-words_model)) table will contain a count for each text and every word.\n",
    "\n",
    "All letters are converted to lower case and we filter the words until we only have words with two or more alphanumeric characters (no punctuation).\n",
    "\n",
    "Important parameters are:\n",
    "\n",
    "-  ``max_features`` limits the model to consider only the top *n* most frequent words. More is often better but may be slow to compute.\n",
    "- ``min_df`` and ``max_df`` restrict the words to those that occur in a certain proportion of texts.\n",
    "\n",
    "For example, setting these to the values 0.2 and 0.8, respectively, restricts the model to words that are in at least 20%, and at most 80% of the texts. This removes rare words on the one hand, and ignores highly frequent words such as [function words](http://en.wikipedia.org/wiki/Function_word) on the other.\n",
    "\n",
    "- ``use_idf=True`` turns on [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) weighting; this incorporates not only the total frequency of a word, but also the number of texts in which it appears. This means that words which are frequent in one text but not in others will get a high score, while words that are frequent in all documents receive a lower score.\n",
    "- ``sublinear_tf=True`` is another variation which makes the word frequencies logarithmic.\n",
    "- ``ngram_range``: selects the number of words in each feature; the default of ``(1, 1)`` selects individual words, which is most efficient. Higher numbers result in more informative features, but also requires a lot more memory. Bigrams, i.e., ``ngram_range=(2, 2)`` is a good trade-off and yields features with 2 consecutive words such as ``the man`` and ``man walks`` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transform() vs fit_transform()\n",
    "\n",
    "\n",
    "fit means to fit the model to the data being provided. This is where the model \"learns\" from the data.\n",
    "\n",
    "transform means to transform the data (produce model outputs) according to the fitted model.\n",
    "\n",
    "fit_transform means to do both - Fit the model to the data, then transform the data according to the fitted model. Calling fit_transform is a convenience to avoid needing to call fit and transform sequentially on the same input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scif.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1d7ba1163633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Get a list of all filenames in the 'train/' folder,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# and add their text to the BOW table 'X'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhistpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scif.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistpd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Filename\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scif.csv'"
     ]
    }
   ],
   "source": [
    "# Set up a simple BOW model\n",
    "# vectorizer is a variable that contains the instructions on how to read the text\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "        input='filename', lowercase=True, token_pattern=r'\\b[-\\w]{1,}\\b',\n",
    "        min_df=0.2, max_df=0.8, max_features=10000,\n",
    "        use_idf=True, sublinear_tf=False, ngram_range=(2, 3)) # two args min and max value\n",
    "\n",
    "# max features refers to analyse the top 10000 ngrams. So it will count all the ngrams\n",
    "# and give you the top 10000\n",
    "# Get a list of all filenames in the 'train/' folder,\n",
    "# and add their text to the BOW table 'X'.\n",
    "histpd = pandas.read_csv(\"scif.csv\")\n",
    "filenames = list(histpd[\"Filename\"])\n",
    "\n",
    "#filenames = os.listdir('train/')\n",
    "\n",
    "ft = []\n",
    "\n",
    "for a in filenames:\n",
    "    if a in os.listdir('train/'):\n",
    "        ft.append(a)\n",
    "\n",
    "# X = vectorizer.fit_transform(['train/' + a for a in filenames])\n",
    "X = vectorizer.fit_transform(['train/' + a for a in filenames])\n",
    "\n",
    "# X contains the read text\n",
    "\n",
    "# .fit_transform() Learn the vocabulary dictionary and return document-term matrix.\n",
    "# vectorizer.fit_transform(...) collects the vocabulary of words/ngrams (the fit part),\n",
    "# and converts the texts to a table of numbers (the transform part).\n",
    "# the result of that is put in the variable X, which is just a \n",
    "# table of numbers, but the vectorizer knows which columns refer to which words/ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "At initialization, vectorizer is empty. after you fit it on a corpus (line 10), yes, it has a list of cleaned ngrams and it knows how to convert a text to a vector of counts referring to those ngrams. When we initialise it we are just telling vectorizer how to read the text and what to store. \n",
    "\n",
    "the same thing happens later with the classifier. it is only trained when you use .fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7868)\t0.008856784837524083\n",
      "  (0, 1960)\t0.00810655012565561\n",
      "  (0, 9086)\t0.007681541472414307\n",
      "  (0, 2660)\t0.007728465070618337\n",
      "  (0, 8875)\t0.006177808743427838\n",
      "  (0, 9939)\t0.008004628214910901\n",
      "  (0, 1480)\t0.007528771673423943\n",
      "  (0, 6740)\t0.008158443346930479\n",
      "  (0, 7824)\t0.005752280285738934\n",
      "  (0, 8170)\t0.008409366937401788\n",
      "  (0, 867)\t0.005975300346631213\n",
      "  (0, 1461)\t0.008336148874818502\n",
      "  (0, 7667)\t0.007281047170496041\n",
      "  (0, 9631)\t0.014762614933369157\n",
      "  (0, 2334)\t0.007266915568671429\n",
      "  (0, 8671)\t0.00619969818586123\n",
      "  (0, 5905)\t0.008158443346930479\n",
      "  (0, 3749)\t0.00824636662084735\n",
      "  (0, 3843)\t0.006749536573347323\n",
      "  (0, 1013)\t0.00619969818586123\n",
      "  (0, 6400)\t0.0074103934874451384\n",
      "  (0, 5948)\t0.008836319533194492\n",
      "  (0, 5509)\t0.007543803697719905\n",
      "  (0, 1968)\t0.013701299006592513\n",
      "  (0, 4372)\t0.007558889121859051\n",
      "  :\t:\n",
      "  (611, 4007)\t0.007800223501485249\n",
      "  (611, 5742)\t0.025919504957380803\n",
      "  (611, 4170)\t0.0024582513463978608\n",
      "  (611, 1553)\t0.043544197302768435\n",
      "  (611, 7591)\t0.004940849945768101\n",
      "  (611, 7353)\t0.009814759852619363\n",
      "  (611, 2028)\t0.01949682442238847\n",
      "  (611, 49)\t0.010716573451522594\n",
      "  (611, 8901)\t0.02920614309214672\n",
      "  (611, 2709)\t0.004069510822946175\n",
      "  (611, 335)\t0.016418271649119805\n",
      "  (611, 5675)\t0.011654484542352073\n",
      "  (611, 7023)\t0.052434840669697835\n",
      "  (611, 5421)\t0.07366368891553492\n",
      "  (611, 6071)\t0.004710701017397265\n",
      "  (611, 404)\t0.010623206932888892\n",
      "  (611, 508)\t0.004100269096561117\n",
      "  (611, 620)\t0.0015735065268641674\n",
      "  (611, 7999)\t0.001824252405457756\n",
      "  (611, 2643)\t0.010691398039859355\n",
      "  (611, 9611)\t0.021847991258798966\n",
      "  (611, 8738)\t0.027110794034586442\n",
      "  (611, 3535)\t0.00200828914628467\n",
      "  (611, 4171)\t0.011322938376689799\n",
      "  (611, 3835)\t0.002046271982323933\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the vector representation\n",
    "---------------------------------\n",
    "To see what the vector representation looks like, we will look at how a simple [example](https://en.wikipedia.org/wiki/It_was_a_dark_and_stormy_night) is transformed to the BOW representation.\n",
    "\n",
    "Notice that because of the parameters before, many words are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ('It was a dark and stormy night; '\n",
    "        'the rain fell in torrents -- except at occasional intervals, '\n",
    "        'when it was checked by a violent gust of wind which swept '\n",
    "        'up the streets (for it is in London that our scene lies), '\n",
    "        'rattling along the housetops, and fiercely agitating the scanty '\n",
    "        'flame of the lamps that struggled against the darkness.')\n",
    "with open('darkandstormy.txt', 'w', encoding='utf8') as out:\n",
    "    out.write(text)\n",
    "vec = vectorizer.transform(['darkandstormy.txt']) # Transform documents to document-term matrix. No training.\n",
    "\n",
    "# we get back a large vector with a value for each possible word.\n",
    "# show a table with only the non-zero items in the vector: \n",
    "feature_names = vectorizer.get_feature_names() # gets the actual words\n",
    "pandas.DataFrame([(feature_names[b], (a, b), vec[a, b])\n",
    "                  # from dictionary giving the word (value) , then using the (key) index\n",
    "                        for a, b in zip(*vec.nonzero())], \n",
    "                       columns=['word', 'index', 'weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-zero items will exist for a document because it stores the words from previous documents it has read. \n",
    "\n",
    "The Index column refers to the index of the file and the index of the word, or more generally, the row and column in the array/matrix/table\n",
    "\n",
    "[`zip()`](https://www.geeksforgeeks.org/zip-in-python/) maps the separate values into one tuple containing all aspects of the item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get label for each text\n",
    "----------------------\n",
    "The genre of each text, whether a text is a \"success\" or a \"failure\" (based on download counts), and some other information, is specified in a separate metadata file.\n",
    "\n",
    "Note that in the original data, a text may have multiple genres, and an arbitrary genre was picked in this case. In a more careful study the single most appropriate genre would have to be selected by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename,Label,Genre,Fold,Success,Title,Author,Language,DownloadCount\n",
      "11228.txt,Chesnutt_TheMarrowOfTradition,Historical,1,SUCCESS,the marrow of tradition,\"chesnutt, charles w. (charles waddell), 1858-1932\",en,147\n",
      "11413.txt,Doyle_TheRefugeesATaleOfTwoContinents,Historical,1,SUCCESS,the refugees a tale of two continents,\"doyle, arthur conan, sir, 1859-1930\",en,129\n",
      "17221.txt,Defoe_HistoryOfThePlagueInLondon,Historical,1,SUCCESS,history of the plague in london,\"defoe, daniel, 1661?-1731\",en,247\n",
      "1880.txt,Cooper_PathfinderOrTheInlandSea,Historical,1,SUCCESS,\"pathfinder; or, the inland sea\",\"cooper, james fenimore, 1789-1851\",en,185\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 lines to see what the metadata looks like:\n",
    "for line in list(open('historical.csv', encoding='utf8'))[:5]:\n",
    "    print(line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the books are assigned to 5 \"folds\" randomly, to divide the corpus into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'504.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b730ac1ff8eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# collect the labels we want to predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create an abbreviated label \"Author_Title\" for each text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-b730ac1ff8eb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# collect the labels we want to predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create an abbreviated label \"Author_Title\" for each text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '504.txt'"
     ]
    }
   ],
   "source": [
    "# Load the data; metadata.index will be the filename.\n",
    "metadata = pandas.read_csv('historical.csv', index_col='Filename', encoding='utf8')\n",
    "labels = dict(zip(metadata.index, metadata['Success']))\n",
    "\n",
    "# collect the labels we want to predict\n",
    "y = numpy.array([labels[a] for a in filenames])\n",
    "print(y)\n",
    "# Create an abbreviated label \"Author_Title\" for each text\n",
    "authors = dict(zip(metadata.index, metadata['Author']))\n",
    "titles = dict(zip(metadata.index, metadata['Title']))\n",
    "abbrtitles = ['%s_%s' % (authors[a].split(',')[0].title(),\n",
    "        titles[a][:15].title()) for a in filenames]\n",
    "\n",
    "type(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier\n",
    "------------------\n",
    "\n",
    "You can try different values for the parameter ``C``.\n",
    "This parameter controls the level of regularization;\n",
    "with higher values, the model will take more edge cases\n",
    "(datapoints close to datapoints of other classes) into account.\n",
    "This will give better scores on data that is similar to the training data,\n",
    "but if the training data is not representative, it may result in more errors.\n",
    "\n",
    "regularisation: ignore certain outliers. \n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "[Logistic Regression Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)in its basic form, models a binary dependent variable (win/lose, pass/fail). Models the probability of a certain event existing.\n",
    "The function that converts log-odds to probability is the logistic function. \n",
    "\n",
    "The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (which group does a new data point belong to?) (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.\n",
    "\n",
    "As opposed to a linear regression model which fits the data to a straight line, a logistic regression model fits the data to a logistic regression curve, which looks like an asymptote. |S|\n",
    "\n",
    "Multinomial regression uses a dependent variable with more than two categories. \n",
    "\n",
    "## lbgfs\n",
    "\n",
    "solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "\n",
    "Algorithm to use in the optimization problem: \n",
    "\n",
    "- For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
    "\n",
    "- For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
    "\n",
    "- ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n",
    "\n",
    "- ‘liblinear’ and ‘saga’ also handle L1 penalty\n",
    "\n",
    "- ‘saga’ also supports ‘elasticnet’ penalty\n",
    "\n",
    "[lbfgs](https://en.wikipedia.org/wiki/Limited-memory_BFGS): The algorithm's target problem is to minimize f(x) over unconstrained values (x can have any value) of the real-vector x where f is a differentiable scalar function (continuous). Trying to find the minimum value of the function. In this case, reducing the amount of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.754967 %\n"
     ]
    }
   ],
   "source": [
    "# Use the \"Fold\" column to split the training data into a train and test set.\n",
    "# These folds were chosen in such a way that the labels are well balanced\n",
    "# and all works by each other only occur in a single fold.\n",
    "folds = dict(zip(metadata.index, metadata['Fold']))\n",
    "\n",
    "# sss contains the split up sample\n",
    "sss = sklearn.model_selection.PredefinedSplit([folds[a] for a in filenames])\n",
    "# This returns train_index and test_index, which are arrays with indices of the datapoints\n",
    "# that should be used for training and testing, respectively.\n",
    "\n",
    "\n",
    "# Train a linear classifier and predict the genre of the items in the test set. \n",
    "# predict the genre?\n",
    "# as before, giving instructions on how to read\n",
    "clf = sklearn.linear_model.LogisticRegression(C=3, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# X refers to the document-term matrix with the list of indices of cleaned ngrams\n",
    "# X is a table of word counts, one document per row, one word/ngram per column.\n",
    "# X[train_index] then selects only the word counts for the books that should be \n",
    "# trained on, without the books in test_index.\n",
    "# y refers to the numpy array of title names of the texts in the corpus and whether they fail or succeed\n",
    "\n",
    "# when we say sss.split(X,y) we are saying split X and y based on the instructions stored in sss\n",
    "for train_index, test_index in sss.split(X, y): \n",
    "    clf.fit(X[train_index], y[train_index]) # fit => learning from the data then it predicts.\n",
    "    pred = clf.predict(X[test_index]) \n",
    "    prob = clf.predict_proba(X[test_index])\n",
    "print('Overall accuracy: %g %%' % sklearn.metrics.accuracy_score(y[test_index], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FAILURE', 'SUCCESS'], dtype='<U7')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the classifier\n",
    "-----------------------\n",
    "The breakdown shows that not all genres are predicted as well;\n",
    "the f-score column is the most important.\n",
    "\n",
    "In the [confusion matrix](http://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "we can see which genres were mistaken most often.\n",
    "The columns hold the number of times the model predicted a genre,\n",
    "while the rows show the true genres.\n",
    "\n",
    "specifically, the diagonal from the top left to the bottom right are the counts of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     FAILURE       0.73      0.80      0.77        76\n",
      "     SUCCESS       0.78      0.71      0.74        75\n",
      "\n",
      "    accuracy                           0.75       151\n",
      "   macro avg       0.76      0.75      0.75       151\n",
      "weighted avg       0.76      0.75      0.75       151\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>FAILURE</th>\n",
       "      <th>SUCCESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAILURE</th>\n",
       "      <td>61</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUCCESS</th>\n",
       "      <td>22</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FAILURE  SUCCESS\n",
       "FAILURE  61       15     \n",
       "SUCCESS  22       53     "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y[test_index], pred))\n",
    "pandas.DataFrame(sklearn.metrics.confusion_matrix(y[test_index], pred),\n",
    "                 index=clf.classes_, columns=clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words are most strongly associated with each genre?\n",
    "---------------------------------------------------------\n",
    "For each genre, the top 10 words most strongly linked to each genre are shown.\n",
    "\n",
    "The words are ordered by the weight of the model for each genre combined with the average frequency of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">FAILURE</th>\n",
       "      <th colspan=\"2\" halign=\"left\">SUCCESS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can t</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>she had</td>\n",
       "      <td>0.0437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i ve</td>\n",
       "      <td>-0.0251</td>\n",
       "      <td>that she</td>\n",
       "      <td>0.0257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the young</td>\n",
       "      <td>-0.0251</td>\n",
       "      <td>you re</td>\n",
       "      <td>0.0181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>she said</td>\n",
       "      <td>-0.0206</td>\n",
       "      <td>the king</td>\n",
       "      <td>0.0175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the girl</td>\n",
       "      <td>-0.0204</td>\n",
       "      <td>the very</td>\n",
       "      <td>0.0149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i shall</td>\n",
       "      <td>-0.0201</td>\n",
       "      <td>i said</td>\n",
       "      <td>0.0148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>won t</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>looked at</td>\n",
       "      <td>0.0147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>as she</td>\n",
       "      <td>-0.0185</td>\n",
       "      <td>the wind</td>\n",
       "      <td>0.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the boys</td>\n",
       "      <td>-0.0178</td>\n",
       "      <td>there s</td>\n",
       "      <td>0.0118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ain t</td>\n",
       "      <td>-0.0152</td>\n",
       "      <td>among the</td>\n",
       "      <td>0.0115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FAILURE            SUCCESS           \n",
       "   word       score   word       score  \n",
       "1       can t -0.0363    she had  0.0437\n",
       "2        i ve -0.0251   that she  0.0257\n",
       "3   the young -0.0251     you re  0.0181\n",
       "4    she said -0.0206   the king  0.0175\n",
       "5    the girl -0.0204   the very  0.0149\n",
       "6     i shall -0.0201     i said  0.0148\n",
       "7       won t -0.0190  looked at  0.0147\n",
       "8      as she -0.0185   the wind  0.0127\n",
       "9    the boys -0.0178    there s  0.0118\n",
       "10      ain t -0.0152  among the  0.0115"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the weights of the classifier and take top 10 items\n",
    "topfeatures = {}\n",
    "avgfreq = numpy.squeeze(numpy.asarray(X.mean(axis=0)))\n",
    "if len(clf.classes_) > 2:\n",
    "    for n, target in enumerate(clf.classes_):\n",
    "        top10 = numpy.argsort(clf.coef_[n] * avgfreq)[::-1][:10]\n",
    "        topfeatures[target] = pandas.DataFrame({\n",
    "                'word': [feature_names[m] for m in top10],\n",
    "                'score': (clf.coef_[n] * avgfreq)[top10]},\n",
    "                index=range(1, 11))\n",
    "else:\n",
    "    # in case of a binary classification, negative weights are for the first class,\n",
    "    # positive weights for the second\n",
    "    top10 = numpy.argsort(clf.coef_[0] * avgfreq)[:10]\n",
    "    topfeatures[clf.classes_[0]] = pandas.DataFrame({\n",
    "            'word': [feature_names[m] for m in top10],\n",
    "            'score': (clf.coef_[0] * avgfreq)[top10]},\n",
    "            index=range(1, 11))\n",
    "    top10 = numpy.argsort(clf.coef_[0] * avgfreq)[::-1][:10]\n",
    "    topfeatures[clf.classes_[1]] = pandas.DataFrame({\n",
    "            'word': [feature_names[m] for m in top10],\n",
    "            'score': (clf.coef_[0] * avgfreq)[top10]},\n",
    "            index=range(1, 11))\n",
    "pandas.concat(topfeatures, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can the model predict labels for new texts?\n",
    "---------------------------------------------\n",
    "\n",
    "If you're done experimenting with the model parameters, we can load new texts that the model has never seen before, and see what it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     FAILURE       0.78      0.70      0.74        80\n",
      "     SUCCESS       0.72      0.80      0.76        79\n",
      "\n",
      "    accuracy                           0.75       159\n",
      "   macro avg       0.75      0.75      0.75       159\n",
      "weighted avg       0.75      0.75      0.75       159\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>FAILURE</th>\n",
       "      <th>SUCCESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAILURE</th>\n",
       "      <td>56</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUCCESS</th>\n",
       "      <td>16</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FAILURE  SUCCESS\n",
       "FAILURE  56       24     \n",
       "SUCCESS  16       63     "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we now evaluate on an external held-out set,\n",
    "# we can use everything as training data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Transform the new files to the format of the existing BOW table\n",
    "newfiles = os.listdir('test/')\n",
    "X1 = vectorizer.transform(['test/' + a for a in newfiles])\n",
    "y1 = numpy.array([labels[a] for a in newfiles])\n",
    "pred = clf.predict(X1)\n",
    "prob = clf.predict_proba(X1)\n",
    "\n",
    "# Evaluate\n",
    "print(sklearn.metrics.classification_report(y1, pred))\n",
    "pandas.DataFrame(sklearn.metrics.confusion_matrix(y1, pred),\n",
    "                       index=clf.classes_,\n",
    "                       columns=clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which books were the hardest to classify?\n",
    "-----------------------------------------\n",
    "\n",
    "The following table lists the Adventure books ordered by how confident the classifier is; i.e., ordered by the probability for the most likely label. A probability ranges from 0 to 1.\n",
    "\n",
    "In the table after this the probabilities for each possible genre are given, and the probabilities of each row add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>prob</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30680.txt</th>\n",
       "      <td>Olin, Richard</td>\n",
       "      <td>All Day Wednesday</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>FAILURE</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24945.txt</th>\n",
       "      <td>Mcneile, H. C. (Herman Cyr...</td>\n",
       "      <td>Mufti</td>\n",
       "      <td>0.5016</td>\n",
       "      <td>FAILURE</td>\n",
       "      <td>FAILURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15963.txt</th>\n",
       "      <td>Emerson, Ralph Waldo, 1803...</td>\n",
       "      <td>May-Day And Other Pieces</td>\n",
       "      <td>0.5030</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>FAILURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027.txt</th>\n",
       "      <td>Grey, Zane, 1872-1939</td>\n",
       "      <td>The Lone Star Ranger, A Ro...</td>\n",
       "      <td>0.5110</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>FAILURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16217.txt</th>\n",
       "      <td>Black, William, 1841-1898</td>\n",
       "      <td>Prince Fortunatus</td>\n",
       "      <td>0.5138</td>\n",
       "      <td>FAILURE</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944.txt</th>\n",
       "      <td>Chekhov, Anton Pavlovich, ...</td>\n",
       "      <td>The Witch And Other Stories</td>\n",
       "      <td>0.9087</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789.txt</th>\n",
       "      <td>Penrose, Margaret</td>\n",
       "      <td>The Motor Girls On A Tour</td>\n",
       "      <td>0.9096</td>\n",
       "      <td>FAILURE</td>\n",
       "      <td>FAILURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695.txt</th>\n",
       "      <td>Chesterton, G. K. (Gilbert...</td>\n",
       "      <td>The Man Who Was Thursday, ...</td>\n",
       "      <td>0.9190</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290.txt</th>\n",
       "      <td>Flaubert, Gustave, 1821-1880</td>\n",
       "      <td>Salammbo</td>\n",
       "      <td>0.9554</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25873.txt</th>\n",
       "      <td>Penrose, Margaret</td>\n",
       "      <td>The Motor Girls On Crystal...</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>FAILURE</td>\n",
       "      <td>FAILURE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author                         Title                           \\\n",
       "30680.txt                  Olin, Richard              All Day Wednesday   \n",
       "24945.txt  Mcneile, H. C. (Herman Cyr...                          Mufti   \n",
       "15963.txt  Emerson, Ralph Waldo, 1803...       May-Day And Other Pieces   \n",
       "1027.txt           Grey, Zane, 1872-1939  The Lone Star Ranger, A Ro...   \n",
       "16217.txt      Black, William, 1841-1898              Prince Fortunatus   \n",
       "...                                  ...                            ...   \n",
       "1944.txt   Chekhov, Anton Pavlovich, ...    The Witch And Other Stories   \n",
       "2789.txt               Penrose, Margaret      The Motor Girls On A Tour   \n",
       "1695.txt   Chesterton, G. K. (Gilbert...  The Man Who Was Thursday, ...   \n",
       "1290.txt    Flaubert, Gustave, 1821-1880                       Salammbo   \n",
       "25873.txt              Penrose, Margaret  The Motor Girls On Crystal...   \n",
       "\n",
       "           prob   actual   predicted  \n",
       "30680.txt  0.5004  FAILURE  SUCCESS   \n",
       "24945.txt  0.5016  FAILURE  FAILURE   \n",
       "15963.txt  0.5030  SUCCESS  FAILURE   \n",
       "1027.txt   0.5110  SUCCESS  FAILURE   \n",
       "16217.txt  0.5138  FAILURE  SUCCESS   \n",
       "...           ...      ...       ...  \n",
       "1944.txt   0.9087  SUCCESS  SUCCESS   \n",
       "2789.txt   0.9096  FAILURE  FAILURE   \n",
       "1695.txt   0.9190  SUCCESS  SUCCESS   \n",
       "1290.txt   0.9554  SUCCESS  SUCCESS   \n",
       "25873.txt  0.9632  FAILURE  FAILURE   \n",
       "\n",
       "[159 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pandas.DataFrame([\n",
    "            (authors[a].title(), titles[a].title(), p, labels[a], b)\n",
    "        for a, b, p in zip(newfiles, pred, prob.max(axis=1))],\n",
    "        index=newfiles,\n",
    "        columns=['Author', 'Title', 'prob', 'actual', 'predicted'])\n",
    "result.sort_values(by='prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>FAILURE</th>\n",
       "      <th>SUCCESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30680.txt</th>\n",
       "      <td>0.4996</td>\n",
       "      <td>0.5004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24945.txt</th>\n",
       "      <td>0.5016</td>\n",
       "      <td>0.4984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15963.txt</th>\n",
       "      <td>0.5030</td>\n",
       "      <td>0.4970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027.txt</th>\n",
       "      <td>0.5110</td>\n",
       "      <td>0.4890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16217.txt</th>\n",
       "      <td>0.4862</td>\n",
       "      <td>0.5138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944.txt</th>\n",
       "      <td>0.0913</td>\n",
       "      <td>0.9087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789.txt</th>\n",
       "      <td>0.9096</td>\n",
       "      <td>0.0904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695.txt</th>\n",
       "      <td>0.0810</td>\n",
       "      <td>0.9190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290.txt</th>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.9554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25873.txt</th>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.0368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAILURE  SUCCESS\n",
       "30680.txt  0.4996   0.5004 \n",
       "24945.txt  0.5016   0.4984 \n",
       "15963.txt  0.5030   0.4970 \n",
       "1027.txt   0.5110   0.4890 \n",
       "16217.txt  0.4862   0.5138 \n",
       "...            ...      ...\n",
       "1944.txt   0.0913   0.9087 \n",
       "2789.txt   0.9096   0.0904 \n",
       "1695.txt   0.0810   0.9190 \n",
       "1290.txt   0.0446   0.9554 \n",
       "25873.txt  0.9632   0.0368 \n",
       "\n",
       "[159 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show probabilities of all labels for each novel (in case of 2 labels, this doesn't give extra information)\n",
    "x = pandas.DataFrame(prob, index=newfiles, columns=clf.classes_)\n",
    "x.loc[x.max(axis=1).sort_values().index, :]  # order from lowest to highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words are most strongly associated with each label?\n",
    "---------------------------------------------------------\n",
    "For each label, the top 10 words most strongly linked to each label are shown.\n",
    "\n",
    "The words are ordered by the weight of the model for each label combined with the average frequency of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">FAILURE</th>\n",
       "      <th colspan=\"2\" halign=\"left\">SUCCESS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>she said</td>\n",
       "      <td>-0.0413</td>\n",
       "      <td>she had</td>\n",
       "      <td>0.0515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can t</td>\n",
       "      <td>-0.0380</td>\n",
       "      <td>that she</td>\n",
       "      <td>0.0232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the girl</td>\n",
       "      <td>-0.0275</td>\n",
       "      <td>you re</td>\n",
       "      <td>0.0204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the young</td>\n",
       "      <td>-0.0239</td>\n",
       "      <td>looked at</td>\n",
       "      <td>0.0181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the boys</td>\n",
       "      <td>-0.0215</td>\n",
       "      <td>i said</td>\n",
       "      <td>0.0168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>as she</td>\n",
       "      <td>-0.0208</td>\n",
       "      <td>the house</td>\n",
       "      <td>0.0153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>won t</td>\n",
       "      <td>-0.0203</td>\n",
       "      <td>the wind</td>\n",
       "      <td>0.0147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the doctor</td>\n",
       "      <td>-0.0146</td>\n",
       "      <td>there s</td>\n",
       "      <td>0.0142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i m</td>\n",
       "      <td>-0.0133</td>\n",
       "      <td>kind of</td>\n",
       "      <td>0.0136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>her eyes</td>\n",
       "      <td>-0.0130</td>\n",
       "      <td>so that</td>\n",
       "      <td>0.0135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FAILURE             SUCCESS           \n",
       "   word        score   word       score  \n",
       "1     she said -0.0413    she had  0.0515\n",
       "2        can t -0.0380   that she  0.0232\n",
       "3     the girl -0.0275     you re  0.0204\n",
       "4    the young -0.0239  looked at  0.0181\n",
       "5     the boys -0.0215     i said  0.0168\n",
       "6       as she -0.0208  the house  0.0153\n",
       "7        won t -0.0203   the wind  0.0147\n",
       "8   the doctor -0.0146    there s  0.0142\n",
       "9          i m -0.0133    kind of  0.0136\n",
       "10    her eyes -0.0130    so that  0.0135"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the weights of the classifier and take top 10 items\n",
    "topfeatures = {}\n",
    "avgfreq = numpy.squeeze(numpy.asarray(X.mean(axis=0)))\n",
    "if len(clf.classes_) > 2:\n",
    "    for n, target in enumerate(clf.classes_):\n",
    "        top10 = numpy.argsort(clf.coef_[n] * avgfreq)[::-1][:10]\n",
    "        topfeatures[target] = pandas.DataFrame({\n",
    "                'word': [feature_names[m] for m in top10],\n",
    "                'score': (clf.coef_[n] * avgfreq)[top10]},\n",
    "                index=range(1, 11))\n",
    "else:\n",
    "    # in case of a binary classification, negative weights are for the first class,\n",
    "    # positive weights for the second\n",
    "    top10 = numpy.argsort(clf.coef_[0] * avgfreq)[:10]\n",
    "    topfeatures[clf.classes_[0]] = pandas.DataFrame({\n",
    "            'word': [feature_names[m] for m in top10],\n",
    "            'score': (clf.coef_[0] * avgfreq)[top10]},\n",
    "            index=range(1, 11))\n",
    "    top10 = numpy.argsort(clf.coef_[0] * avgfreq)[::-1][:10]\n",
    "    topfeatures[clf.classes_[1]] = pandas.DataFrame({\n",
    "            'word': [feature_names[m] for m in top10],\n",
    "            'score': (clf.coef_[0] * avgfreq)[top10]},\n",
    "            index=range(1, 11))\n",
    "pandas.concat(topfeatures, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "text1 =\"# Sort the weights of the classifier and take top 10 itemstopfeatures = {}avgfreq = numpy.squeeze(numpy.asarray(X.mean(axis=0)))if len(clf.classes_) > 2:for n, target in enumerate(clf.classes_):top10 = numpy.argsort(clf.coef_[n] * avgfreq)[::-1][:10]topfeatures[target] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[n] * avgfreq)[top10]},index=range(1, 11))else:# in case of a binary classification, negative weights are for the first class,# positive weights for the secondtop10 = numpy.argsort(clf.coef_[0] * avgfreq)[:10]topfeatures[clf.classes_[0]] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[0] * avgfreq)[top10]},index=range(1, 11))top10 = numpy.argsort(clf.coef_[0] * avgfreq)[::-1][:10]topfeatures[clf.classes_[1]] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[0] * avgfreq)[top10]},index=range(1, 11))pandas.concat(topfeatures, axis=1)\"\n",
    "text2 =\"# Sort the weights of the classifier and take top 10 itemstopfeatures = {}avgfreq = numpy.squeeze(numpy.asarray(X.mean(axis=0)))if len(clf.classes_) > 2:for n, target in enumerate(clf.classes_):top10 = numpy.argsort(clf.coef_[n] * avgfreq)[::-1][:10]topfeatures[target] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[n] * avgfreq)[top10]},index=range(1, 11))else:# in case of a binary classification, negative weights are for the first class,# positive weights for the secondtop10 = numpy.argsort(clf.coef_[0] * avgfreq)[:10]topfeatures[clf.classes_[0]] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[0] * avgfreq)[top10]},index=range(1, 11))top10 = numpy.argsort(clf.coef_[0] * avgfreq)[::-1][:10]topfeatures[clf.classes_[1]] = pandas.DataFrame({'word': [feature_names[m] for m in top10],'score': (clf.coef_[0] * avgfreq)[top10]},index=range(1, 11))pandas.concat(topfeatures, axis=1)\"\n",
    "\n",
    "if text1 == text2: \n",
    "    print(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
